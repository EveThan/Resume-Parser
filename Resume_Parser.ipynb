{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWutbPkAPZEj"
      },
      "source": [
        "# Install the required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFOZtxV3PWnw",
        "outputId": "48fbea61-32ad-4896-e4f4-4bf01452303f"
      },
      "outputs": [],
      "source": [
        "!pip install tika==1.24\n",
        "!pip install spacy==2.2.3\n",
        "!pip install scikit_learn==0.23.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyjOQxVGtcKv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "import logging\n",
        "import spacy\n",
        "import tika\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "from spacy.gold import GoldParse\n",
        "from spacy.scorer import Scorer\n",
        "from spacy.util import minibatch, compounding\n",
        "tika.initVM()\n",
        "from tika import parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzvzulKbZJTl"
      },
      "source": [
        "# Define all the functions we will need"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G56iDI0btzNB"
      },
      "source": [
        "## Convert the input data or input file into a format that SpaCy accepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6o88J00tyn_"
      },
      "outputs": [],
      "source": [
        "def convert_data_to_spacy(JSON_File):\n",
        "\n",
        "  '''\n",
        "  # Parameter\n",
        "  # JSON_File: The input file that is in json format\n",
        "\n",
        "  # Return value\n",
        "  # training_data: A list of data that is in a format that SpaCy accepts. It is a list that contains tuples. The tuples will contain 2 items.\n",
        "                   The first item is the text of the whole resume. The second item is a dictionary where the key is the string \"entities\" and the value\n",
        "                   is a yet another list of tuples. The tuple has the format (start_point, end_point, label). \n",
        "                   Therefore, the value of the key \"entities\" will contain tuples for all words that are labelled in the given text of a resume.\n",
        "  # Returns None if there is something wrong with opening and extracting data from the input file\n",
        "  '''\n",
        "\n",
        "  try:\n",
        "    training_data = []\n",
        "    lines = []\n",
        "\n",
        "    # Open the input json file and encode it to bytes\n",
        "    with open(JSON_File, 'r', encoding = 'utf-8') as f:\n",
        "      lines = f.readlines()\n",
        "\n",
        "    # Go through each line of the input file and extract the texts\n",
        "    for line in lines:\n",
        "      data = json.loads(line)\n",
        "      text = data['content']\n",
        "      \n",
        "      entities = []\n",
        "      # Extract the entities (start point, end point, and label of a word)\n",
        "      for annotation in data['annotation']:\n",
        "        # We can have more than 1 label for a word but there is only one start point and one end point for a word\n",
        "        # So keep point as a single value but labels as a list\n",
        "        point = annotation['points'][0]\n",
        "        labels = annotation['label']\n",
        "        \n",
        "        # Make sure that labels is a list\n",
        "        if not isinstance(labels, list):\n",
        "          labels = [labels]\n",
        "\n",
        "        # If a word has 2 labels, 2 entity tuples will be recorded for the word\n",
        "        # SpaCy needs the end point to be 1 more than the point where the word really ends\n",
        "        for label in labels:\n",
        "          entities.append((point['start'], point['end']+1, label))\n",
        "\n",
        "      training_data.append((text, {\"entities\" : entities}))\n",
        "\n",
        "    print(\"Successfully converted the input data into SpaCy format.\\n\")\n",
        "\n",
        "    return training_data\n",
        "\n",
        "  except Exception as e:\n",
        "    print(\"Unable to process the file: \" + JSON_File + \"\\n\" + \"Error: \" + str(e))\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UlZXVuo0XcO"
      },
      "source": [
        "## Check if model already exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9bL99Q6xxfy"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def check_model(model_name):\n",
        "\n",
        "  '''\n",
        "  # Parameter\n",
        "  # model_name: The name of the model that we want to load if it exists\n",
        "\n",
        "  # Return value\n",
        "  # model_name: The name of the model that is loaded successfully\n",
        "  # Returns None if there is something wrong in loading the model specified by model_name. It is likely because the model doesn't exist.\n",
        "  '''\n",
        "\n",
        "  try:\n",
        "    model = spacy.load(model_name)\n",
        "    print(\"The model exists and is loaded successfully.\\n\")\n",
        "    \n",
        "    return model_name\n",
        "  \n",
        "  except Exception as e:\n",
        "    print(\"Model is not loaded successfully. Make sure to check if the model exists.\\n\")\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmErxxlX5pnK"
      },
      "source": [
        "## Build a SpaCy model (or update the model if it exists) and train it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVltIk6ZQSC1",
        "outputId": "fef775be-a3ba-459e-ac85-53fff5772362"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg7pjclP1lOF"
      },
      "outputs": [],
      "source": [
        "def build_spacy_model(training_data, model_name):\n",
        "  \n",
        "  '''\n",
        "  # Parameter\n",
        "  # training_data: The list of data that is converted into SpaCy format. This will be what the function convert_data_to_spacy returns.\n",
        "  # model_name: The name of the model that we want to load if it exists (not None)\n",
        "\n",
        "  # Return value\n",
        "  # spacy_model: The SpaCy model that is built or updated \n",
        "  '''\n",
        "\n",
        "  if model_name is not None:\n",
        "    nlp = spacy.load(model_name)\n",
        "    print(\"The model \" + model_name + \" exists and is loaded successfully.\\n\")\n",
        "\n",
        "  else:\n",
        "    # Create a SpaCy model that is based on the English language if model_name is None\n",
        "    nlp = spacy.blank(\"en\")\n",
        "    print(\"A new, blank SpaCy model based on English is created.\\n\")\n",
        "\n",
        "  # Create a built-in component \"ner\" and add it to the pipeline\n",
        "  # ner is a built-in SpaCy pipeline component for recognizing entities\n",
        "  if \"ner\" not in nlp.pipe_names:\n",
        "    ner = nlp.create_pipe(\"ner\")\n",
        "    nlp.add_pipe(ner, last = True)\n",
        "\n",
        "  else:\n",
        "    ner = nlp.get_pipe(\"ner\")\n",
        "\n",
        "  # Get the labels from the training data\n",
        "  for _, annotations_dict in training_data:\n",
        "    for entity in annotations_dict.get('entities'):\n",
        "      # entity contains 3 elements: start_point, end_point, and label of a word\n",
        "      ner.add_label(entity[2])\n",
        "\n",
        "  # Disable other components of the pipeline before training\n",
        "  other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
        "  with nlp.disable_pipes(*other_pipes):\n",
        "    if model_name is None:\n",
        "      # begin_training returns an optimizer\n",
        "      optimizer = nlp.begin_training()\n",
        "    \n",
        "    for iter in range(500):\n",
        "      print(\"Starting iteration \" + str(iter))\n",
        "\n",
        "      random.shuffle(training_data)\n",
        "      losses = {}\n",
        "      for text, annotations_dict in training_data:\n",
        "        try:\n",
        "          # Apply dropout so that it is harder for the model to memorize the data\n",
        "          nlp.update([text], [annotations_dict], drop = 0.2, sgd = optimizer, losses = losses)\n",
        "        except Exception as e:\n",
        "          pass\n",
        "\n",
        "      print(\"Losses: \", losses)\n",
        "\n",
        "  # Save the model on Google Drive\n",
        "  nlp.to_disk(\"model\")\n",
        "\n",
        "  return nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6Uhvx6pSzoa"
      },
      "source": [
        "## Convert PDF to text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bstnZTN9QLXa"
      },
      "outputs": [],
      "source": [
        "def convert_pdf_to_text(dir):\n",
        "\n",
        "  '''\n",
        "  # Parameter\n",
        "  # dir: The directory where the PDF files are \n",
        "\n",
        "  # Return value\n",
        "  # output: A list that contains the texts of all the PDF files that are converted \n",
        "  '''\n",
        "\n",
        "  output = []\n",
        "  for dirpath, dirnames, filenames in os.walk(dir):\n",
        "    for file in filenames:\n",
        "      path_to_pdf_file = os.path.join(dirpath, file)\n",
        "      [path, extension] = os.path.splitext(path_to_pdf_file)\n",
        "\n",
        "      # Can only work with PDF files\n",
        "      if(extension == \".pdf\"):\n",
        "        # Get the content in text form\n",
        "        pdf_content_text = parser.from_file(path_to_pdf_file, service = \"text\")\n",
        "        # Append the text content into the output list\n",
        "        output.append(pdf_content_text['content'])\n",
        "\n",
        "  return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91wJ8SS2WKmm"
      },
      "source": [
        "## Use the model to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8-TFk0FTmnq"
      },
      "outputs": [],
      "source": [
        "def predict(path):\n",
        "\n",
        "  '''\n",
        "  # Parameter\n",
        "  # path: The path where the PDF files are\n",
        "\n",
        "  # Return value\n",
        "  # output: A dictionary that contains the entities recognized. The key corresponds to the hash index of a specific \n",
        "            entity.\n",
        "  '''\n",
        "\n",
        "  output = {}\n",
        "  nlp = spacy.load(\"model\")\n",
        "  test_data = convert_pdf_to_text(path)\n",
        "  \n",
        "  for text in test_data:\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    doc = nlp(text)\n",
        "    print(\"doc: \", doc)\n",
        "\n",
        "    #print(\"entities: \", doc.ents)\n",
        "    for ent in doc.ents:\n",
        "      print(f'{ent.label_.upper():{30}}-{ent,text}')\n",
        "      output[ent.label_.upper()] = ent.text\n",
        "\n",
        "  return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3-ktMlkYnHk"
      },
      "source": [
        "# Run all the functions defined above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_U6V0z2cZsp",
        "outputId": "8b8066b4-bc74-4fe4-e512-851bab4ea60b"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/My Drive/Resume Parser/input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw5B6moEXn-l",
        "outputId": "3888b01a-bfc5-45d8-f02b-7a960c6a71f4"
      },
      "outputs": [],
      "source": [
        "# Convert the training data to SpaCy format\n",
        "train = convert_data_to_spacy(\"Entity Recognition in Resumes.json\")\n",
        "\n",
        "print(\"Data is converted to SpaCy format\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoLxi-USeNCI",
        "outputId": "97fcd193-e0f5-4846-c4f4-077d6d107f94"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/My Drive/Resume Parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD_abUoKeT2A",
        "outputId": "887a9d87-4be7-4b51-83b5-d264a9e905a4"
      },
      "outputs": [],
      "source": [
        "# Load model if exists \n",
        "model = check_model(\"model\")\n",
        "\n",
        "# TRAIN\n",
        "# Build a new model or update existing model \n",
        "model = build_spacy_model(train, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDJ6_vuzeA-X",
        "outputId": "9c51a36e-75aa-4666-c61e-c3f178620b7d"
      },
      "outputs": [],
      "source": [
        "# TEST\n",
        "# Use the model to predict\n",
        "output = predict(\"/content/drive/My Drive/Resume Parser/test\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Resume Parser",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
